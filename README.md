# Trump Speeches NLP Chatbot

[![Tests](https://github.com/JustaKris/Trump-Rally-Speeches-NLP-Chatbot/actions/workflows/python-tests.yml/badge.svg)](https://github.com/JustaKris/Trump-Rally-Speeches-NLP-Chatbot/actions/workflows/python-tests.yml)
[![Linting](https://github.com/JustaKris/Trump-Rally-Speeches-NLP-Chatbot/actions/workflows/python-lint.yml/badge.svg)](https://github.com/JustaKris/Trump-Rally-Speeches-NLP-Chatbot/actions/workflows/python-lint.yml)
[![Security](https://github.com/JustaKris/Trump-Rally-Speeches-NLP-Chatbot/actions/workflows/security-audit.yml/badge.svg)](https://github.com/JustaKris/Trump-Rally-Speeches-NLP-Chatbot/actions/workflows/security-audit.yml)
[![Documentation](https://img.shields.io/badge/docs-GitHub%20Pages-blue)](https://justakris.github.io/Trump-Rally-Speeches-NLP-Chatbot/)
[![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue.svg)](https://www.python.org/downloads/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A full-stack NLP application built from the ground up â€” combining retrieval-augmented generation, hybrid search, multi-model sentiment analysis, and AI-powered topic extraction into a production-ready FastAPI service. Features pluggable LLM providers (Gemini, OpenAI, Claude), comprehensive testing, and automated deployment pipelines.

## What's Inside

### The AI Stack

- **RAG Q&A System** â€” Natural language questions over 300,000+ words using ChromaDB vector storage, MPNet embeddings (768d), and hybrid search combining semantic similarity with BM25 keyword matching
- **Multi-Provider LLM Integration** â€” Pluggable architecture supporting Gemini, OpenAI GPT, and Anthropic Claude with a unified interface and lazy-loaded dependencies
- **Smart Confidence Scoring** â€” Multi-factor calculation weighing semantic similarity, answer consistency, context coverage, and entity presence
- **Entity Analytics Engine** â€” Extract entities with sentiment analysis, track co-occurrences, and map contextual associations across documents
- **Advanced Sentiment Analysis** â€” Ensemble approach combining FinBERT (sentiment), RoBERTa (emotion detection), and LLM-generated contextual interpretation
- **AI-Powered Topic Clustering** â€” DBSCAN semantic clustering with sentence-transformers + LLM-generated labels and summaries

### The Engineering Side

- **FastAPI Backend** â€” 12+ RESTful endpoints with async handling, dependency injection, Pydantic validation, and comprehensive error handling
- **Modular RAG Architecture** â€” Separated concerns with dedicated components for search, confidence calculation, entity analysis, and document loading
- **Production Deployment** â€” Multi-stage Docker builds, Azure Web App hosting, GitHub Actions CI/CD, automated testing and security scanning
- **Developer Experience** â€” Type hints throughout, structured logging (JSON for prod, pretty for dev), comprehensive documentation with MkDocs, and 65%+ test coverage
- **Modern Python Tooling** â€” uv for dependency management, Ruff for linting/formatting, pytest with parametrized tests, mypy for type checking

## Try It Live

The API is deployed on Azure and ready to explore:

- **[Interactive Web App](https://trump-speeches-nlp-chatbot.azurewebsites.net)** â€” Try the RAG system, sentiment analysis, and topic extraction
- **[API Docs (Swagger)](https://trump-speeches-nlp-chatbot.azurewebsites.net/docs)** â€” Interactive API playground
- **[API Docs (ReDoc)](https://trump-speeches-nlp-chatbot.azurewebsites.net/redoc)** â€” Clean, readable documentation
- **[Full Documentation](https://justakris.github.io/Trump-Rally-Speeches-NLP-Chatbot/)** â€” Complete guides, architecture diagrams, and API reference

> **Note:** Azure free tier apps cold-start after inactivity. First request may take 1-2 minutes to wake the service.

## How It Works

### RAG System Architecture

Built a modular question-answering system over 35 political speeches (300,000+ words) with these components:

**Core Services:**
- **`services/rag_service.py`** â€” Orchestrates RAG pipeline, manages ChromaDB, coordinates components
- **`services/llm/`** â€” Pluggable LLM abstraction layer supporting multiple providers (Gemini, OpenAI, Anthropic)
  - **`base.py`** â€” Abstract LLMProvider interface
  - **`factory.py`** â€” Factory pattern with lazy imports for optional providers
  - **`gemini.py`** â€” Google Gemini implementation
  - **`openai.py`** â€” OpenAI GPT models (optional dependency)
  - **`anthropic.py`** â€” Anthropic Claude models (optional dependency)

**Modular RAG Components** (`services/rag/`):
- **`search_engine.py`** â€” Hybrid search combining semantic (MPNet 768d), BM25 keyword, and cross-encoder reranking
- **`confidence.py`** â€” Multi-factor confidence scoring (retrieval quality, consistency, coverage, entity mentions)
- **`entity_analyzer.py`** â€” Entity extraction with sentiment analysis, speech coverage, and co-occurrence analytics
- **`document_loader.py`** â€” Document chunking (2048 chars, 150 overlap) with metadata tracking
- **`models.py`** â€” Pydantic data models for type-safe RAG operations

**RAG API Endpoints:**
- `POST /rag/ask` â€” Ask natural language questions with AI-generated answers
- `POST /rag/search` â€” Semantic search over indexed documents
- `GET /rag/stats` â€” Vector database statistics and health check
- `POST /rag/index` â€” Index/re-index documents

### ğŸ“ Traditional NLP Endpoints

**API Layer** (`api/`):
- **`routes_chatbot.py`** â€” RAG question-answering endpoints
- **`routes_nlp.py`** â€” Traditional NLP analysis endpoints
- **`routes_health.py`** â€” Health checks and system status
- **`dependencies.py`** â€” Dependency injection for services

**Core Services:**
- **`services/nlp_service.py`** â€” Word frequency, n-gram analysis
- **`services/sentiment_service.py`** â€” Enhanced AI-powered sentiment analysis with emotion detection and contextual interpretation
- **`services/topic_service.py`** â€” AI-powered topic extraction with semantic clustering and LLM-generated summaries

**Additional Endpoints:**
- `POST /analyze/sentiment` â€” Multi-model sentiment analysis (FinBERT + RoBERTa emotions + Gemini interpretation)
- `POST /analyze/words` â€” Word frequency
- `POST /analyze/topics` â€” AI-powered topic extraction with semantic clustering and contextual analysis
- `POST /analyze/ngrams` â€” N-gram analysis

### ğŸ“Š Demo Dataset

35 political rally speech transcripts (2019-2020) totaling 300,000+ words â€” indexed in ChromaDB for RAG queries. The dataset demonstrates the system's ability to handle real-world political text with nuanced language.

### ğŸ¨ Interactive Web Interface

Single-page application at the root (`/`) for testing all API features including the RAG Q&A system.

### ğŸ““ Analysis Notebooks

Jupyter notebooks showcasing statistical NLP and exploratory data analysis techniques on the speech corpus.

## Technical Highlights

### AI/ML Engineering

- **RAG Systems**: End-to-end retrieval-augmented generation with ChromaDB vector database
- **LLM Integration**: Multi-provider abstraction layer with Gemini, OpenAI GPT, and Anthropic Claude support
- **Design Patterns**: Factory pattern with lazy imports, abstract base classes, dependency injection
- **Semantic Search**: Hybrid search combining dense embeddings (MPNet) and sparse retrieval (BM25)
- **Model Selection**: Cross-encoder reranking for precision optimization
- **Confidence Scoring**: Multi-factor confidence calculation for answer quality assessment
- **Transformer Models**: FinBERT sentiment analysis, sentence-transformers for embeddings
- **Entity Analytics**: NER-based entity extraction with sentiment and co-occurrence analysis

### Backend Engineering

- **API Development**: Production-grade FastAPI with 12+ RESTful endpoints, async request handling, modular route organization
- **Vector Databases**: ChromaDB with persistent storage, smart deduplication, efficient querying
- **Modular Architecture**: Separated concerns with dedicated services for search, confidence, entities, and document loading
- **Configuration Management**: Pydantic Settings with type validation and environment-based config
- **Production Logging**: JSON-formatted logs for cloud platforms, colored output for development
- **Error Handling**: Graceful fallbacks, comprehensive exception handling, structured error responses
- **Performance**: Efficient chunking (2048 chars), hybrid search, cross-encoder reranking
- **Type Safety**: Full Pydantic validation, Python 3.11+ type hints throughout
- **Dependency Injection**: Clean service initialization and testable architecture

### DevOps & Quality

- **Containerization**: Multi-stage Docker builds, non-root user, health checks
- **CI/CD**: GitHub Actions with automated testing, security scanning, code quality gates
- **Testing**: pytest with 65%+ coverage, unit and integration tests, parametrized test cases, modular component testing
- **Code Quality**: Ruff (linting + formatting), mypy (type checking) enforced via CI
- **Security**: pip-audit for vulnerability scanning, bandit for security analysis
- **Documentation**: Comprehensive MkDocs site, API docs via Swagger/ReDoc, inline docstrings
- **Observability**: Structured logging, health endpoints, startup configuration display

## What You Can Ask

The RAG system handles complex queries like:

- *"What economic policies were discussed in the speeches?"*
- *"How many times was Biden mentioned and in what context?"*
- *"What did the speaker say about immigration?"*
- *"Compare the themes between 2019 and 2020 speeches"*

It retrieves relevant chunks via hybrid search, analyzes entities and sentiment, calculates multi-factor confidence scores, and generates coherent answers with source attribution.

## What's New

### LLM Provider Abstraction (Recent)

- **Multi-Provider Support**: Pluggable architecture supporting Gemini, OpenAI GPT, and Anthropic Claude
- **Model-Agnostic Configuration**: Single configuration interface for all providers (`LLM_API_KEY`, `LLM_MODEL_NAME`)
- **Factory Pattern**: Lazy imports with optional dependencies for clean provider switching
- **Type-Safe Interface**: Abstract base class ensuring consistent LLM behavior across providers
- **Easy Extension**: Add new providers by implementing the `LLMProvider` interface

### Enhanced AI-Powered NLP Features

- **Multi-Model Sentiment Analysis**:
  - FinBERT for sentiment classification (positive/negative/neutral)
  - RoBERTa emotion detection (anger, joy, fear, sadness, surprise, disgust)
  - Gemini LLM for contextual interpretation explaining WHY the models produced their results
  - Clean UI with AI interpretation as the focal point, compact score visualization
- **Semantic Topic Extraction**:
  - Sentence-transformer embeddings for semantic similarity
  - DBSCAN clustering for intelligent topic grouping
  - LLM-generated cluster labels and comprehensive topic summaries
  - Interactive snippets showing topic occurrences in context
- **Centralized Configuration**: All NLP parameters (thresholds, model names, excluded verbs) configurable via environment variables

### Modular RAG Architecture (Code Refactoring)

- **Component Separation**: Extracted RAG functionality into dedicated, testable modules
  - `SearchEngine`: Hybrid search with semantic, BM25, and cross-encoder reranking
  - `ConfidenceCalculator`: Multi-factor confidence scoring
  - `EntityAnalyzer`: Entity extraction, sentiment, and co-occurrence analysis
  - `DocumentLoader`: Smart chunking with metadata tracking
- **Improved Testability**: 65%+ test coverage with component-level unit tests
- **Type Safety**: Pydantic models for all RAG data structures
- **Maintainability**: Clear separation of concerns, easier to extend and debug

### Production-Ready Logging

- **Dual-format logging**: JSON for production/cloud, colored for development
- **Automatic environment detection**: Zero configuration needed
- **Cloud-native**: Works with Azure Application Insights, CloudWatch, ELK stack
- **Smart filtering**: Suppresses noisy third-party library logs

### Professional Configuration Management

- **Type-safe settings**: Pydantic validation catches errors at startup
- **Environment-based**: Full `.env` file support for local and cloud deployment
- **Flexible**: Support for multiple LLM providers (Gemini, OpenAI, Anthropic)
- **Cloud-ready**: Seamless Azure/AWS deployment with environment variables

### Performance & Reliability

- **Smart deduplication**: Prevents re-indexing existing ChromaDB chunks
- **100x faster re-indexing**: Skip embedding computation for existing documents
- **Clean logs**: No more duplicate ID warnings or telemetry errors
- **Dependency injection**: Better testability and cleaner initialization

[See full changelog](docs/CHANGELOG.md)

## Get Started

### What You Need

- Python 3.11 or newer
- [uv](https://docs.astral.sh/uv/getting-started/installation/) (modern Python package manager)
- An LLM API key (grab a free one from [Google Gemini](https://ai.google.dev/) â€” it's the default provider)
  - Or use [OpenAI](https://platform.openai.com/api-keys) / [Anthropic](https://console.anthropic.com/) if you prefer

### Setup

1. **Install dependencies**

   ```powershell
   uv sync
   ```

2. **Configure LLM Provider**

   The project supports multiple LLM providers with a model-agnostic configuration approach.

   **Option A: Google Gemini (Default)**

   Create a `.env` file in the project root:
   ```bash
   # LLM Provider Configuration
   LLM_PROVIDER=gemini
   LLM_API_KEY=your_gemini_api_key_here
   LLM_MODEL_NAME=gemini-2.0-flash-exp
   
   # Optional: Adjust LLM parameters
   LLM_TEMPERATURE=0.7
   LLM_MAX_OUTPUT_TOKENS=2048
   ```

   **Option B: OpenAI**

   ```powershell
   # Install OpenAI support
   uv sync --group llm-openai
   ```

   Update `.env`:
   ```bash
   LLM_PROVIDER=openai
   LLM_API_KEY=sk-your_openai_api_key_here
   LLM_MODEL_NAME=gpt-4o-mini
   LLM_TEMPERATURE=0.7
   LLM_MAX_OUTPUT_TOKENS=2048
   ```

   **Option C: Anthropic (Claude)**

   ```powershell
   # Install Anthropic support
   uv sync --group llm-anthropic
   ```

   Update `.env`:
   ```bash
   LLM_PROVIDER=anthropic
   LLM_API_KEY=sk-ant-your_anthropic_api_key_here
   LLM_MODEL_NAME=claude-3-5-sonnet-20241022
   LLM_TEMPERATURE=0.7
   LLM_MAX_OUTPUT_TOKENS=2048
   ```

   **Install All Providers:**
   ```powershell
   uv sync --group llm-all
   ```

3. **Start the FastAPI server**

   ```powershell
   uv run uvicorn src.api:app --reload
   ```

4. **Access the application**
   - **Web Interface:** <https://trump-speeches-nlp-chatbot.azurewebsites.net>
   - **API Docs:** <https://trump-speeches-nlp-chatbot.azurewebsites.net/docs>
   - **ReDoc:** <https://trump-speeches-nlp-chatbot.azurewebsites.net/redoc>

### Try the RAG System

**Web Interface:** Navigate to the RAG tab and ask a question

**API Example:**
```powershell
curl -X POST http://localhost:8000/rag/ask `
  -H "Content-Type: application/json" `
  -d '{"question": "What was said about the economy?", "top_k": 5}'
```

**Python Example:**
```python
import requests

response = requests.post(
    "http://localhost:8000/rag/ask",
    json={"question": "What economic policies were discussed?", "top_k": 5}
)
print(response.json()["answer"])
```

### Alternative: Docker

**Note:** Add your Gemini API key to the Dockerfile or pass it as an environment variable.

### Run with Docker

1. **Build the Docker image**

   ```powershell
   docker build -t trump-speeches-nlp-chatbot .
   ```

2. **Run the container**

   ```powershell
   docker run -d -p 8000:8000 trump-speeches-nlp-chatbot
   ```

3. **Or use Docker Compose**

   ```powershell
   docker-compose up -d
   ```

### View Documentation Site (Optional)

The project includes comprehensive documentation built with MkDocs:

```powershell
# Install documentation dependencies
uv sync --group docs

# Serve documentation site locally (with live reload)
uv run mkdocs serve
```

Then open <http://localhost:8001> to browse the documentation with search and navigation.

**Build static site:**
```powershell
uv run mkdocs build
```

This generates a `site/` folder with the complete static documentation website.

### Explore Analysis Notebooks (Optional)

```powershell
# Install notebook dependencies (includes matplotlib, seaborn, plotly, etc.)
uv sync --group notebooks
uv run jupyter lab
```

Navigate to `notebooks/` to explore statistical NLP analysis and visualizations.

## Testing & Code Quality

Built with testing in mind â€” comprehensive test suite with pytest, automated CI/CD, and modern Python tooling.

### Run Tests

```powershell
# Install dev dependencies
uv sync --group dev

# Run all tests with coverage
uv run pytest

# Run only unit tests
uv run pytest -m unit

# Run only integration tests
uv run pytest -m integration

# Generate HTML coverage report
uv run pytest --cov=src --cov-report=html
```

### Code Quality Checks

```powershell
# Format code
uv run ruff format src/

# Lint code
uv run flake8 src/

# Sort imports
uv run isort src/

# Type checking
uv run mypy src/

# Run all checks
uv run ruff format src/ && uv run ruff check src/ && uv run pytest
```

### CI/CD Pipeline

The project uses modular GitHub Actions workflows for continuous integration:
- âœ… **Automated testing** on Python 3.11, 3.12 ([`python-tests.yml`](.github/workflows/python-tests.yml))
- âœ… **Code quality** â€” Ruff linting and formatting ([`python-lint.yml`](.github/workflows/python-lint.yml))
- âœ… **Type checking** â€” Mypy static analysis ([`python-typecheck.yml`](.github/workflows/python-typecheck.yml))
- âœ… **Security scanning** â€” Bandit and pip-audit ([`security-audit.yml`](.github/workflows/security-audit.yml))
- âœ… **Documentation** â€” Auto-deploy to GitHub Pages ([`docs.yml`](.github/workflows/docs.yml))
- âœ… **Docker builds** â€” Automated image builds ([`build-push-docker.yml`](.github/workflows/build-push-docker.yml))

For detailed testing documentation, see [`docs/howto/testing.md`](docs/howto/testing.md).

## ğŸ“¦ Dependencies

## ğŸ“¦ Core Dependencies

**RAG & LLM:**
- `chromadb` â€” Vector database for embeddings
- `google-generativeai` â€” Gemini LLM integration (default provider)
- `openai` â€” OpenAI GPT models (optional: `uv sync --group llm-openai`)
- `anthropic` â€” Anthropic Claude models (optional: `uv sync --group llm-anthropic`)
- `sentence-transformers` â€” MPNet embeddings (768d)
- `rank-bm25` â€” Keyword search for hybrid retrieval
- `langchain` â€” Text splitting utilities

**NLP & ML:**
- `transformers` + `torch` â€” FinBERT sentiment analysis, RoBERTa emotion detection
- `nltk` â€” Text preprocessing
- `scikit-learn` â€” DBSCAN clustering, cosine similarity

**API & Infrastructure:**
- `fastapi` â€” REST API framework
- `uvicorn` â€” ASGI server
- `pydantic` â€” Data validation

See `pyproject.toml` for complete dependency list.

## ğŸ’¡ Project Structure

```
Trump-Rally-Speeches-NLP-Chatbot/
â”‚
â”œâ”€â”€ src/                          # Production API code
â”‚   â”œâ”€â”€ api.py                   # FastAPI with RAG & NLP endpoints
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ rag_service.py       # â­ RAG orchestration
â”‚   â”‚   â”œâ”€â”€ llm/                 # â­ Pluggable LLM providers
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py          #    Abstract LLMProvider interface
â”‚   â”‚   â”‚   â”œâ”€â”€ factory.py       #    Factory pattern with lazy imports
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.py        #    Google Gemini implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ openai.py        #    OpenAI GPT (optional)
â”‚   â”‚   â”‚   â””â”€â”€ anthropic.py     #    Anthropic Claude (optional)
â”‚   â”‚   â”œâ”€â”€ rag/                 # Modular RAG components
â”‚   â”‚   â”‚   â”œâ”€â”€ search_engine.py #    Hybrid search
â”‚   â”‚   â”‚   â”œâ”€â”€ confidence.py    #    Confidence scoring
â”‚   â”‚   â”‚   â”œâ”€â”€ entity_analyzer.py #  Entity extraction
â”‚   â”‚   â”‚   â””â”€â”€ document_loader.py #  Document chunking
â”‚   â”‚   â”œâ”€â”€ sentiment_service.py # Multi-model sentiment
â”‚   â”‚   â””â”€â”€ topic_service.py     # Semantic topic extraction
â”‚   â”œâ”€â”€ models.py                # FinBERT sentiment analysis
â”‚   â”œâ”€â”€ preprocessing.py         # Text preprocessing
â”‚   â””â”€â”€ utils.py                 # Data loading utilities
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Donald Trump Rally Speeches/  # 35 speech transcripts
â”‚   â””â”€â”€ chromadb/                     # Vector database persistence
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html               # Web interface
â”‚
â”œâ”€â”€ notebooks/                   # Exploratory analysis
â”œâ”€â”€ tests/                       # pytest test suite
â”œâ”€â”€ docs/                        # Documentation (MkDocs site)
â”‚   â”œâ”€â”€ index.md                # Docs homepage
â”‚   â”œâ”€â”€ guides/                 # Getting started guides
â”‚   â”œâ”€â”€ howto/                  # Task-oriented guides
â”‚   â””â”€â”€ reference/              # Technical reference
â”œâ”€â”€ mkdocs.yml                   # Documentation site config
â””â”€â”€ pyproject.toml               # Dependencies
```

## ï¿½ Documentation

**ğŸ“˜ [Full Documentation Site](https://justakris.github.io/Trump-Rally-Speeches-NLP-Chatbot/)** â€” Complete guides, tutorials, and API reference

### View Documentation Locally (Optional)

```powershell
# Install docs dependencies
uv sync --group docs

# Serve docs with live reload (use port 8001 to avoid API conflict)
uv run mkdocs serve --dev-addr localhost:8001
```

Then open <http://localhost:8001> in your browser.

For more information on working with the documentation, see the [Documentation Guide](https://justakris.github.io/Trump-Rally-Speeches-NLP-Chatbot/howto/documentation/).

## ï¿½ğŸ“„ License & Attribution

This repository is for educational and portfolio purposes. The speech transcripts are publicly available data used for demonstrative NLP analysis.

**Technologies Used:**

- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [FinBERT](https://huggingface.co/ProsusAI/finbert) for sentiment analysis
- [Plotly](https://plotly.com/python/) for interactive visualizations
- [uv](https://docs.astral.sh/uv/) for dependency management

---

## Get in Touch

**Kristiyan Bonev**

- GitHub: [@JustaKris](https://github.com/JustaKris)
- Email: k.s.bonev@gmail.com

Built this from scratch to explore modern NLP techniques and production ML deployment. Dive into the code, try the API, or reach out if you want to chat about RAG systems, LLM integration, or anything else!
